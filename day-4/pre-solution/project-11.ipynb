{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20_000\n",
    "line_length = 80\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of reviews\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in each review\n",
    "for i in range(10):\n",
    "    print(i, len(x_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment, positive or negative\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping dictionaries\n",
    "word_to_id = imdb.get_word_index()\n",
    "id_to_word = {v:k for k,v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[0] = '<START>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review(index):\n",
    "    return ' '.join([id_to_word[max(0, idx-3)] for idx in x_train[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all rows the same length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=line_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ************************************************\n",
    "# CHANGE THE EPOCHS, BELOW, TO GET HIGHER ACCURACY\n",
    "# ************************************************\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1,figsize=(12,7))\n",
    "ax1.plot(history.epoch, history.history['loss'], marker='^', color='purple')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss', color='purple')\n",
    "ax1.tick_params('y', colors='purple')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "plt.plot(history.epoch, history.history['acc'], marker='+', color='green', label='train')\n",
    "ax2.set_ylim(0,1)\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "plt.plot(history.epoch, history.history['val_acc'], marker='*', color='red', label='validation')\n",
    "ax3.set_ylim(0,1)\n",
    "\n",
    "fig.suptitle('imdb sentiment reviews');\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.get_layer('embedding_1').get_weights()\n",
    "embedding[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space.\n",
    "\n",
    "For instance, \"coconut\" and \"polar bear\" are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But \"kitchen\" and \"dinner\" are related words, so they should be embedded close to each other.\n",
    "\n",
    "The most common application of this layer is for text processing. Let's see a simple example. Our training set consists only of two phrases:\n",
    "\n",
    "Hope to see you soon\n",
    "Nice to see you again\n",
    "\n",
    "So we can encode these phrases by assigning each word a unique integer number (by order of appearance in our training dataset for example). Then our phrases could be rewritten as:\n",
    "\n",
    "[0, 1, 2, 3, 4]\n",
    "[5, 1, 2, 3, 6]\n",
    "\n",
    "Now imagine we want to train a network whose first layer is an embeding layer. In this case, we should initialize it as follows:\n",
    "\n",
    "Embedding(7, 2, input_length=5)\n",
    "\n",
    "The first argument (7) is the number of distinct words in the training set. The second argument (2) indicates the size of the embedding vectors. The input_length argumet, of course, determines the size of each input sequence.\n",
    "\n",
    "Once the network has been trained, we can get the weights of the embedding layer, which in this case will be of size (7, 2) and can be thought as the table used to map integers to embedding vectors:\n",
    "\n",
    "+------------+------------+\n",
    "|   index    |  Embedding |\n",
    "+------------+------------+\n",
    "|     0      | [1.2, 3.1] |\n",
    "|     1      | [0.1, 4.2] |\n",
    "|     2      | [1.0, 3.1] |\n",
    "|     3      | [0.3, 2.1] |\n",
    "|     4      | [2.2, 1.4] |\n",
    "|     5      | [0.7, 1.7] |\n",
    "|     6      | [4.1, 2.0] |\n",
    "+------------+------------+\n",
    "\n",
    "So according to these embeddings, our second training phrase will be represented as:\n",
    "\n",
    "[[0.7, 1.7], [0.1, 4.2], [1.0, 3.1], [0.3, 2.1], [4.1, 2.0]]\n",
    "\n",
    "- Word embeddings provide a dense representation of words and their relative meanings.\n",
    "- They are an improvement over sparse representations used in simpler bag of word model representations.\n",
    "- Word embeddings can be learned from text data and reused among projects. They can also be learned as part of fitting a neural network on text data.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict sentiment from reviews\n",
    "bad = \"this movie was terrible and bad\"\n",
    "good = \"i really liked the movie and had fun\"\n",
    "for review in [good,bad]:\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = sequence.pad_sequences([tmp], maxlen=line_length) \n",
    "    print(\"{}. Sentiment: {:.2f}\".format(  review, model.predict(np.array([tmp_padded][0]))[0][0]  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
